{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "import torch.nn.functional as F\n",
    "#from run_generation import top_k_top_p_filtering\n",
    "#from run_generation import sample_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1, 50257])\n",
      " time, the world was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spvengat/.local/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "#text = \"Happy Birthday to\"\n",
    "#indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "#tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "#CUDA - Compute Unified Device Architecture\n",
    "#.to() - Sets dtype or device to a tensor\n",
    "#tokens_tensor = tokens_tensor.to('cpu')\n",
    "model.to('cpu')\n",
    "\n",
    "# Predict all tokens\n",
    "#with torch.no_grad():\n",
    "    #outputs = model(tokens_tensor)\n",
    "    #filter_outputs = top_k_top_p_filtering(logits=outputs, top_k=10, top_p=0.5)\n",
    "    #predictions = outputs[0]\n",
    "\n",
    "# Get the predicted next sub-word\n",
    "#predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "#predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "\n",
    "# Print the predicted word\n",
    "#print(predicted_text)\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "        Args:\n",
    "            logits: logits distribution shape (..., vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = torch.tensor(cumulative_probs >= top_p, dtype=torch.uint8)\n",
    "        print(sorted_indices_to_remove.shape)\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        #Zeros_like - creates tensor with zeros same length as input\n",
    "        indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        #indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "def inference(model = GPT2LMHeadModel, enc = GPT2Tokenizer, phrase= '', top_k = 1, top_p = 0.9, length = 1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nsamples = 1\n",
    "    length = length\n",
    "    temperature = 1.2\n",
    "    top_k = top_k\n",
    "    top_p = top_p\n",
    "    batch_size = 1\n",
    "    stop_token = [enc.encoder[x] for x in ('<|endoftext|>', '.', '?', '!')]\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    if length == -1:\n",
    "        length = model.config.n_ctx // 2\n",
    "    elif length > model.config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "\n",
    "    context_tokens = enc.encode(phrase) if phrase else [enc.encoder['<|endoftext|>']]\n",
    "    generated = 0\n",
    "    out = sample_sequence(\n",
    "        model=model, length=length,\n",
    "        context=context_tokens,\n",
    "        start_token=None,\n",
    "        batch_size=batch_size,\n",
    "        temperature=temperature, top_k=top_k, device=device,\n",
    "        top_p=top_p,\n",
    "        stop_token=stop_token\n",
    "    )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    return enc.decode(out[0])\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0,\n",
    "                    device='cuda', top_p=0, stop_token=[]):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        while count < length:\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_top_p_filtering(logits, top_p=top_p, top_k=top_k)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            prev = torch.multinomial(probs, num_samples=1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "            count += 1\n",
    "            if prev in stop_token:\n",
    "                break\n",
    "    return output\n",
    "\n",
    "print(inference(model=model,enc=tokenizer,phrase='Once upon a', length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
